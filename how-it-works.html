<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>How It Works - DeepVision AI</title>
    <link href="https://fonts.googleapis.com/css2?family=Orbitron:wght@500&display=swap" rel="stylesheet">
    <style>
        body {
            margin: 0;
            font-family: 'Orbitron', sans-serif;
            background: linear-gradient(135deg, #001f3f, #8a2be2);
            color: white;
            overflow-x: hidden;
            animation: fadeIn 2s ease-in-out;
        }

        @keyframes fadeIn {
            0% { opacity: 0; }
            100% { opacity: 1; }
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            text-align: center;
        }

        header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 20px;
            border-bottom: 1px solid rgba(255, 255, 255, 0.1);
            opacity: 0;
            animation: fadeInHeader 1s forwards;
        }

        @keyframes fadeInHeader {
            0% { opacity: 0; }
            100% { opacity: 1; }
        }

        header h1 {
            font-size: 2.5rem;
            text-shadow: 0 0 8px rgba(255, 255, 255, 0.7);
        }

        nav a {
            margin: 0 10px;
            color: #fff;
            text-decoration: none;
            font-weight: bold;
            opacity: 0;
            animation: fadeInNav 1.5s forwards;
            animation-delay: 0.5s;
        }

        @keyframes fadeInNav {
            0% { opacity: 0; }
            100% { opacity: 1; }
        }

        .how-it-works-video {
            width: 100%;
            max-width: 800px;
            margin: 20px auto;
            border-radius: 10px;
            box-shadow: 0 0 20px rgba(255, 255, 255, 0.3);
            animation: zoomIn 1.5s ease-out;
        }

        @keyframes zoomIn {
            0% { transform: scale(0.8); opacity: 0; }
            100% { transform: scale(1); opacity: 1; }
        }

        .section {
            margin: 40px 0;
            padding: 20px;
            background: rgba(255, 255, 255, 0.1);
            backdrop-filter: blur(10px);
            border-radius: 15px;
            opacity: 0;
            animation: slideUp 1.5s ease-out forwards;
        }

        @keyframes slideUp {
            0% { transform: translateY(50px); opacity: 0; }
            100% { transform: translateY(0); opacity: 1; }
        }

        .section h2 {
            font-size: 2rem;
            margin-bottom: 15px;
            transform: scale(0.8);
            animation: scaleUp 0.5s ease-out forwards;
        }

        @keyframes scaleUp {
            0% { transform: scale(0.8); opacity: 0; }
            100% { transform: scale(1); opacity: 1; }
        }

        .section p {
            font-size: 1.2rem;
            line-height: 1.6;
        }

        .footer {
            text-align: center;
            padding: 20px;
            font-size: 0.9rem;
            border-top: 1px solid rgba(255, 255, 255, 0.1);
            animation: fadeInFooter 1s forwards;
        }

        @keyframes fadeInFooter {
            0% { opacity: 0; }
            100% { opacity: 1; }
        }

        .footer a {
            color: #8a2be2;
            text-decoration: none;
        }

        .section:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 30px rgba(255, 255, 255, 0.3);
            transition: all 0.3s ease;
        }

    </style>
</head>
<body>
    <header>
        <h1>DeepVision AI</h1>
        <nav>
            <a href="index.html">Home</a>
            <a href="datasets.html">Datasets</a>
            <a href="architecture.html">Architecture</a>
        </nav>
    </header>

    <div class="container">

        <section class="section">
            <h2>Training the Model</h2>
            <p>The DeepVision AI model is trained using a large dataset of both deepfake and pristine videos. We use a combination of supervised learning and transfer learning to fine-tune a ResNext50_32x4d model pre-trained on ImageNet. The model is further enhanced using a Long Short-Term Memory (LSTM) network to capture temporal patterns across video frames, which is essential for deepfake detection.</p>
        </section>

        <section class="section">
            <h2>Data Preprocessing</h2>
            <p>The input videos are preprocessed by extracting individual frames, followed by resizing and normalization. Each frame is passed through the ResNext network to extract high-level feature vectors. These vectors serve as the input for the LSTM network, which processes them in sequence to learn the temporal relationships within the video.</p>
        </section>

        <section class="section">
            <h2>How the Model Works</h2>
            <p>After training, the model uses the learned features to classify new videos as either deepfake or pristine. The ResNext CNN performs frame-level feature extraction, followed by LSTM's sequential analysis to identify patterns over time. The final decision is made by a SoftMax layer that provides a probability score, indicating whether the video is real or fake. Dropout layers and Leaky ReLU activations enhance model performance and generalization.</p>
        </section>

        <section class="section">
            <h2>Model Evaluation</h2>
            <p>The model is evaluated using metrics such as accuracy, precision, recall, and F1-score on a validation dataset. During training, the model's performance is monitored through cross-validation, and hyperparameters like learning rate, batch size, and dropout rates are optimized using grid search or random search.</p>
        </section>
    </div>

    <footer class="footer">
        <p>&copy; 2025 DeepVision AI | <a href="#privacy">Privacy Policy</a> | <a href="#terms">Terms of Service</a></p>
    </footer>
</body>
</html>
